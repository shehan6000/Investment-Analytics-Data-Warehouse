# ============================================================================
# INVESTMENT ANALYTICS DATA WAREHOUSE - GOOGLE COLAB SETUP
# ============================================================================
# Copy this entire code to a new Google Colab notebook and run each cell

# CELL 1: Install Dependencies
# ============================================================================
!pip install streamlit pyngrok sqlalchemy pandas numpy plotly -q

print("‚úÖ All packages installed successfully!")


# CELL 2: Create Main Application File
# ============================================================================
# Using Python to write the file instead of %%writefile

app_code = '''import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sqlalchemy import create_engine, Column, Integer, String, Float, Date, ForeignKey, DateTime
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, relationship
import plotly.express as px
import plotly.graph_objects as go

# Database Schema Definition (Star Schema)
Base = declarative_base()

class DimDate(Base):
    __tablename__ = 'dim_date'
    date_key = Column(Integer, primary_key=True)
    full_date = Column(Date, unique=True, nullable=False)
    year = Column(Integer)
    quarter = Column(Integer)
    month = Column(Integer)
    month_name = Column(String(20))
    day = Column(Integer)
    day_of_week = Column(String(20))
    is_weekend = Column(Integer)
    fiscal_year = Column(Integer)
    fiscal_quarter = Column(Integer)

class DimUser(Base):
    __tablename__ = 'dim_user'
    user_key = Column(Integer, primary_key=True, autoincrement=True)
    user_id = Column(String(50), unique=True, nullable=False)
    user_name = Column(String(100))
    risk_profile = Column(String(20))
    account_type = Column(String(50))
    registration_date = Column(Date)
    country = Column(String(50))

class DimAsset(Base):
    __tablename__ = 'dim_asset'
    asset_key = Column(Integer, primary_key=True, autoincrement=True)
    asset_id = Column(String(50), unique=True, nullable=False)
    asset_name = Column(String(100))
    asset_type = Column(String(50))
    sector = Column(String(50))
    currency = Column(String(10))
    exchange = Column(String(50))
    is_active = Column(Integer, default=1)

class DimRegion(Base):
    __tablename__ = 'dim_region'
    region_key = Column(Integer, primary_key=True, autoincrement=True)
    region_id = Column(String(50), unique=True, nullable=False)
    region_name = Column(String(100))
    continent = Column(String(50))
    market_type = Column(String(50))
    timezone = Column(String(50))

class FactTransactions(Base):
    __tablename__ = 'fact_transactions'
    transaction_key = Column(Integer, primary_key=True, autoincrement=True)
    date_key = Column(Integer, ForeignKey('dim_date.date_key'))
    user_key = Column(Integer, ForeignKey('dim_user.user_key'))
    asset_key = Column(Integer, ForeignKey('dim_asset.asset_key'))
    region_key = Column(Integer, ForeignKey('dim_region.region_key'))
    transaction_type = Column(String(20))
    quantity = Column(Float)
    price_per_unit = Column(Float)
    total_amount = Column(Float)
    commission = Column(Float)
    net_amount = Column(Float)
    timestamp = Column(DateTime)

class FactReturns(Base):
    __tablename__ = 'fact_returns'
    return_key = Column(Integer, primary_key=True, autoincrement=True)
    date_key = Column(Integer, ForeignKey('dim_date.date_key'))
    user_key = Column(Integer, ForeignKey('dim_user.user_key'))
    asset_key = Column(Integer, ForeignKey('dim_asset.asset_key'))
    region_key = Column(Integer, ForeignKey('dim_region.region_key'))
    opening_price = Column(Float)
    closing_price = Column(Float)
    daily_return = Column(Float)
    daily_return_pct = Column(Float)
    volume = Column(Float)
    market_value = Column(Float)

# ETL Functions
class InvestmentETL:
    def __init__(self, engine):
        self.engine = engine
        self.Session = sessionmaker(bind=engine)
    
    def extract_generate_sample_data(self):
        np.random.seed(42)
        start_date = datetime(2023, 1, 1)
        end_date = datetime(2024, 12, 31)
        dates = pd.date_range(start_date, end_date, freq='D')
        
        users = [
            {'user_id': f'U{i:04d}', 'user_name': f'User {i}', 
             'risk_profile': np.random.choice(['Conservative', 'Moderate', 'Aggressive']),
             'account_type': np.random.choice(['Individual', 'Joint', 'Corporate']),
             'country': np.random.choice(['USA', 'UK', 'Japan', 'Germany', 'China'])}
            for i in range(1, 51)
        ]
        
        assets = [
            {'asset_id': 'AAPL', 'asset_name': 'Apple Inc.', 'asset_type': 'Stock', 'sector': 'Technology', 'currency': 'USD', 'exchange': 'NASDAQ'},
            {'asset_id': 'GOOGL', 'asset_name': 'Alphabet Inc.', 'asset_type': 'Stock', 'sector': 'Technology', 'currency': 'USD', 'exchange': 'NASDAQ'},
            {'asset_id': 'MSFT', 'asset_name': 'Microsoft Corp.', 'asset_type': 'Stock', 'sector': 'Technology', 'currency': 'USD', 'exchange': 'NASDAQ'},
            {'asset_id': 'TSLA', 'asset_name': 'Tesla Inc.', 'asset_type': 'Stock', 'sector': 'Automotive', 'currency': 'USD', 'exchange': 'NASDAQ'},
            {'asset_id': 'SPY', 'asset_name': 'S&P 500 ETF', 'asset_type': 'ETF', 'sector': 'Index', 'currency': 'USD', 'exchange': 'NYSE'},
            {'asset_id': 'GLD', 'asset_name': 'Gold ETF', 'asset_type': 'ETF', 'sector': 'Commodity', 'currency': 'USD', 'exchange': 'NYSE'},
            {'asset_id': 'BTC', 'asset_name': 'Bitcoin', 'asset_type': 'Crypto', 'sector': 'Cryptocurrency', 'currency': 'USD', 'exchange': 'Crypto'},
            {'asset_id': 'TLT', 'asset_name': 'Treasury Bond ETF', 'asset_type': 'Bond', 'sector': 'Fixed Income', 'currency': 'USD', 'exchange': 'NYSE'},
        ]
        
        regions = [
            {'region_id': 'NA', 'region_name': 'North America', 'continent': 'Americas', 'market_type': 'Developed', 'timezone': 'EST'},
            {'region_id': 'EU', 'region_name': 'Europe', 'continent': 'Europe', 'market_type': 'Developed', 'timezone': 'CET'},
            {'region_id': 'ASIA', 'region_name': 'Asia Pacific', 'continent': 'Asia', 'market_type': 'Mixed', 'timezone': 'JST'},
            {'region_id': 'LATAM', 'region_name': 'Latin America', 'continent': 'Americas', 'market_type': 'Emerging', 'timezone': 'BRT'},
        ]
        
        return dates, users, assets, regions
    
    def transform_load_dimensions(self, dates, users, assets, regions):
        session = self.Session()
        try:
            for i, date in enumerate(dates):
                dim_date = DimDate(
                    date_key=int(date.strftime('%Y%m%d')),
                    full_date=date.date(),
                    year=date.year,
                    quarter=(date.month - 1) // 3 + 1,
                    month=date.month,
                    month_name=date.strftime('%B'),
                    day=date.day,
                    day_of_week=date.strftime('%A'),
                    is_weekend=1 if date.weekday() >= 5 else 0,
                    fiscal_year=date.year if date.month >= 4 else date.year - 1,
                    fiscal_quarter=((date.month - 1) // 3 + 1)
                )
                session.add(dim_date)
            
            for user_data in users:
                dim_user = DimUser(
                    user_id=user_data['user_id'],
                    user_name=user_data['user_name'],
                    risk_profile=user_data['risk_profile'],
                    account_type=user_data['account_type'],
                    registration_date=datetime(2022, 1, 1).date(),
                    country=user_data['country']
                )
                session.add(dim_user)
            
            for asset_data in assets:
                dim_asset = DimAsset(**asset_data, is_active=1)
                session.add(dim_asset)
            
            for region_data in regions:
                dim_region = DimRegion(**region_data)
                session.add(dim_region)
            
            session.commit()
            return True
        except Exception as e:
            session.rollback()
            st.error(f"Error loading dimensions: {e}")
            return False
        finally:
            session.close()
    
    def load_fact_tables(self):
        session = self.Session()
        try:
            users = session.query(DimUser).all()
            assets = session.query(DimAsset).all()
            regions = session.query(DimRegion).all()
            dates = session.query(DimDate).order_by(DimDate.full_date).all()
            
            for _ in range(500):
                user = np.random.choice(users)
                asset = np.random.choice(assets)
                region = np.random.choice(regions)
                date = np.random.choice(dates[:-30])
                
                transaction_type = np.random.choice(['BUY', 'SELL'])
                quantity = np.random.randint(1, 100)
                price = np.random.uniform(50, 500)
                total_amount = quantity * price
                commission = total_amount * 0.001
                net_amount = total_amount - commission if transaction_type == 'BUY' else total_amount + commission
                
                transaction = FactTransactions(
                    date_key=date.date_key,
                    user_key=user.user_key,
                    asset_key=asset.asset_key,
                    region_key=region.region_key,
                    transaction_type=transaction_type,
                    quantity=quantity,
                    price_per_unit=price,
                    total_amount=total_amount,
                    commission=commission,
                    net_amount=net_amount,
                    timestamp=datetime.combine(date.full_date, datetime.min.time())
                )
                session.add(transaction)
            
            for asset in assets[:5]:
                base_price = np.random.uniform(100, 300)
                for i, date in enumerate(dates):
                    daily_return_pct = np.random.normal(0.0005, 0.02)
                    base_price *= (1 + daily_return_pct)
                    
                    opening_price = base_price * np.random.uniform(0.98, 1.02)
                    closing_price = base_price
                    
                    for user in users[:10]:
                        region = np.random.choice(regions)
                        
                        returns = FactReturns(
                            date_key=date.date_key,
                            user_key=user.user_key,
                            asset_key=asset.asset_key,
                            region_key=region.region_key,
                            opening_price=opening_price,
                            closing_price=closing_price,
                            daily_return=closing_price - opening_price,
                            daily_return_pct=daily_return_pct * 100,
                            volume=np.random.randint(100000, 10000000),
                            market_value=closing_price * np.random.randint(10, 1000)
                        )
                        session.add(returns)
            
            session.commit()
            return True
        except Exception as e:
            session.rollback()
            st.error(f"Error loading fact tables: {e}")
            return False
        finally:
            session.close()

# Streamlit Dashboard
@st.cache_resource
def get_database():
    engine = create_engine('sqlite:///investment_analytics_dw.db', echo=False)
    Base.metadata.create_all(engine)
    
    Session = sessionmaker(bind=engine)
    session = Session()
    existing_data = session.query(DimDate).count()
    session.close()
    
    if existing_data == 0:
        st.info("üîÑ Running ETL Pipeline...")
        etl = InvestmentETL(engine)
        dates, users, assets, regions = etl.extract_generate_sample_data()
        etl.transform_load_dimensions(dates, users, assets, regions)
        etl.load_fact_tables()
        st.success("‚úÖ ETL Pipeline Complete!")
    
    return engine

def main():
    st.set_page_config(page_title="Investment Analytics DW", layout="wide", page_icon="üìä")
    
    engine = get_database()
    
    st.title("üìä Investment Analytics Data Warehouse")
    st.markdown("### Star Schema OLAP Dashboard")
    
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration")
        st.info("**Data Warehouse Schema:** Star Schema\\n\\n"
                "**Fact Tables:**\\n- fact_transactions\\n- fact_returns\\n\\n"
                "**Dimension Tables:**\\n- dim_date\\n- dim_user\\n- dim_asset\\n- dim_region")
    
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "üìà Dashboard", "üíº Transactions", "üìä Returns Analysis", "üîç OLAP Cube", "üóÇÔ∏è Schema"
    ])
    
    with tab1:
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            total_transactions = pd.read_sql("SELECT COUNT(*) as cnt FROM fact_transactions", engine)
            st.metric("Total Transactions", f"{total_transactions['cnt'].iloc[0]:,}")
        
        with col2:
            total_volume = pd.read_sql("SELECT SUM(total_amount) as vol FROM fact_transactions", engine)
            st.metric("Total Volume", f"${total_volume['vol'].iloc[0]:,.2f}")
        
        with col3:
            avg_return = pd.read_sql("SELECT AVG(daily_return_pct) as avg_ret FROM fact_returns", engine)
            st.metric("Avg Daily Return", f"{avg_return['avg_ret'].iloc[0]:.3f}%")
        
        with col4:
            active_users = pd.read_sql("SELECT COUNT(DISTINCT user_key) as cnt FROM fact_transactions", engine)
            st.metric("Active Users", f"{active_users['cnt'].iloc[0]:,}")
        
        st.markdown("---")
        
        col1, col2 = st.columns(2)
        
        with col1:
            query = """
            SELECT d.year, d.month, d.month_name, SUM(ft.total_amount) as volume
            FROM fact_transactions ft
            JOIN dim_date d ON ft.date_key = d.date_key
            GROUP BY d.year, d.month, d.month_name
            ORDER BY d.year, d.month
            """
            df_volume = pd.read_sql(query, engine)
            df_volume['period'] = df_volume['month_name'] + ' ' + df_volume['year'].astype(str)
            
            fig = px.line(df_volume, x='period', y='volume', 
                         title='Transaction Volume Over Time',
                         labels={'volume': 'Volume ($)', 'period': 'Period'})
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            query = """
            SELECT a.asset_type, SUM(ft.total_amount) as volume
            FROM fact_transactions ft
            JOIN dim_asset a ON ft.asset_key = a.asset_key
            GROUP BY a.asset_type
            """
            df_assets = pd.read_sql(query, engine)
            
            fig = px.pie(df_assets, values='volume', names='asset_type',
                        title='Transaction Volume by Asset Type')
            st.plotly_chart(fig, use_container_width=True)
    
    with tab2:
        st.subheader("üíº Transaction Analysis")
        
        col1, col2 = st.columns(2)
        with col1:
            transaction_type = st.selectbox("Transaction Type", ["All", "BUY", "SELL"])
        with col2:
            asset_types = pd.read_sql("SELECT DISTINCT asset_type FROM dim_asset", engine)
            asset_type = st.selectbox("Asset Type", ["All"] + asset_types['asset_type'].tolist())
        
        query = """
        SELECT 
            d.full_date,
            u.user_name,
            a.asset_name,
            a.asset_type,
            r.region_name,
            ft.transaction_type,
            ft.quantity,
            ft.price_per_unit,
            ft.total_amount,
            ft.commission
        FROM fact_transactions ft
        JOIN dim_date d ON ft.date_key = d.date_key
        JOIN dim_user u ON ft.user_key = u.user_key
        JOIN dim_asset a ON ft.asset_key = a.asset_key
        JOIN dim_region r ON ft.region_key = r.region_key
        WHERE 1=1
        """
        
        if transaction_type != "All":
            query += f" AND ft.transaction_type = '{transaction_type}'"
        if asset_type != "All":
            query += f" AND a.asset_type = '{asset_type}'"
        
        query += " ORDER BY d.full_date DESC LIMIT 100"
        
        df_transactions = pd.read_sql(query, engine)
        st.dataframe(df_transactions, use_container_width=True, height=400)
        
        col1, col2 = st.columns(2)
        with col1:
            query = """
            SELECT a.asset_name, SUM(ft.total_amount) as total_volume
            FROM fact_transactions ft
            JOIN dim_asset a ON ft.asset_key = a.asset_key
            GROUP BY a.asset_name
            ORDER BY total_volume DESC
            LIMIT 10
            """
            df_top_assets = pd.read_sql(query, engine)
            
            fig = px.bar(df_top_assets, x='asset_name', y='total_volume',
                        title='Top 10 Assets by Transaction Volume')
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            query = """
            SELECT r.region_name, COUNT(*) as transaction_count
            FROM fact_transactions ft
            JOIN dim_region r ON ft.region_key = r.region_key
            GROUP BY r.region_name
            """
            df_regions = pd.read_sql(query, engine)
            
            fig = px.bar(df_regions, x='region_name', y='transaction_count',
                        title='Transactions by Region')
            st.plotly_chart(fig, use_container_width=True)
    
    with tab3:
        st.subheader("üìä Portfolio Returns Analysis")
        
        query = """
        SELECT 
            a.asset_name,
            AVG(fr.daily_return_pct) as avg_return,
            MAX(fr.daily_return_pct) as max_return,
            MIN(fr.daily_return_pct) as min_return
        FROM fact_returns fr
        JOIN dim_asset a ON fr.asset_key = a.asset_key
        GROUP BY a.asset_name
        """
        df_returns = pd.read_sql(query, engine)
        
        fig = px.bar(df_returns, x='asset_name', y='avg_return',
                    title='Average Daily Returns by Asset',
                    color='avg_return',
                    color_continuous_scale='RdYlGn')
        st.plotly_chart(fig, use_container_width=True)
        
        query = """
        SELECT 
            d.full_date,
            a.asset_name,
            fr.closing_price
        FROM fact_returns fr
        JOIN dim_date d ON fr.date_key = d.date_key
        JOIN dim_asset a ON fr.asset_key = a.asset_key
        ORDER BY a.asset_name, d.full_date
        """
        df_time_returns = pd.read_sql(query, engine)
        
        fig = px.line(df_time_returns, x='full_date', y='closing_price', color='asset_name',
                     title='Asset Price Movement Over Time')
        st.plotly_chart(fig, use_container_width=True)
    
    with tab4:
        st.subheader("üîç OLAP Cube - Multi-Dimensional Analysis")
        
        col1, col2 = st.columns(2)
        with col1:
            dimension1 = st.selectbox("Dimension 1 (Rows)", 
                                     ["Year", "Quarter", "Month", "Asset Type", "Region"])
        with col2:
            dimension2 = st.selectbox("Dimension 2 (Columns)",
                                     ["Asset Type", "Region", "Quarter", "Month", "Year"])
        
        dim_map = {
            "Year": ("d.year", "dim_date d", "ft.date_key = d.date_key"),
            "Quarter": ("CAST(d.quarter AS TEXT)", "dim_date d", "ft.date_key = d.date_key"),
            "Month": ("d.month_name", "dim_date d", "ft.date_key = d.date_key"),
            "Asset Type": ("a.asset_type", "dim_asset a", "ft.asset_key = a.asset_key"),
            "Region": ("r.region_name", "dim_region r", "ft.region_key = r.region_key")
        }
        
        dim1_col, dim1_table, dim1_join = dim_map[dimension1]
        dim2_col, dim2_table, dim2_join = dim_map[dimension2]
        
        query = f"""
        SELECT 
            {dim1_col} as dim1,
            {dim2_col} as dim2,
            SUM(ft.total_amount) as total_volume
        FROM fact_transactions ft
        JOIN {dim1_table} ON {dim1_join}
        JOIN {dim2_table} ON {dim2_join}
        GROUP BY {dim1_col}, {dim2_col}
        """
        
        df_cube = pd.read_sql(query, engine)
        pivot_volume = df_cube.pivot(index='dim1', columns='dim2', values='total_volume')
        
        fig = go.Figure(data=go.Heatmap(
            z=pivot_volume.values,
            x=pivot_volume.columns,
            y=pivot_volume.index,
            colorscale='Viridis'
        ))
        
        fig.update_layout(
            title=f'Transaction Volume: {dimension1} vs {dimension2}',
            xaxis_title=dimension2,
            yaxis_title=dimension1,
            height=500
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    with tab5:
        st.subheader("üóÇÔ∏è Star Schema Design")
        
        st.markdown("""
        ### Data Warehouse Architecture
        
        **Fact Tables:**
        - fact_transactions: Buy/sell transaction data
        - fact_returns: Daily portfolio returns
        
        **Dimension Tables:**
        - dim_date: Time dimension
        - dim_user: User/investor profiles
        - dim_asset: Investment assets
        - dim_region: Geographic regions
        
        **ETL Process:**
        1. Extract: Generate sample data
        2. Transform: Clean and validate
        3. Load: Populate star schema
        """)
        
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("#### Dimension Tables")
            for table in ['dim_date', 'dim_user', 'dim_asset', 'dim_region']:
                count = pd.read_sql(f"SELECT COUNT(*) as cnt FROM {table}", engine)
                st.metric(table, f"{count['cnt'].iloc[0]:,}")
        
        with col2:
            st.markdown("#### Fact Tables")
            for table in ['fact_transactions', 'fact_returns']:
                count = pd.read_sql(f"SELECT COUNT(*) as cnt FROM {table}", engine)
                st.metric(table, f"{count['cnt'].iloc[0]:,}")

if __name__ == "__main__":
    main()
'''

# Write the file
with open('investment_dw_app.py', 'w') as f:
    f.write(app_code)

print("‚úÖ Application file created: investment_dw_app.py")


# CELL 3: Setup ngrok with Authentication Token
# ============================================================================
from pyngrok import ngrok, conf
import getpass

# Get ngrok auth token from user
print("üìù Please enter your ngrok authentication token")
print("   Get your free token from: https://dashboard.ngrok.com/get-started/your-authtoken")
print()

ngrok_token = getpass.getpass("Enter your ngrok token: ")

# Set ngrok auth token
conf.get_default().auth_token = ngrok_token

print("‚úÖ ngrok authentication configured!")
print()


# CELL 4: Run Streamlit App with ngrok
# ============================================================================
import subprocess
import time
from pyngrok import ngrok

# Kill any existing Streamlit processes
!pkill -9 streamlit

# Start Streamlit in the background
process = subprocess.Popen(
    ["streamlit", "run", "investment_dw_app.py", "--server.port", "8501"],
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE
)

# Wait for Streamlit to start
print("‚è≥ Starting Streamlit server...")
time.sleep(10)

# Create ngrok tunnel
public_url = ngrok.connect(8501)

print("=" * 70)
print("üéâ Investment Analytics Data Warehouse is now running!")
print("=" * 70)
print(f"\nüåê Public URL: {public_url}")
print(f"\nüìä Open this URL in your browser to access the dashboard")
print("\n‚ö†Ô∏è  Keep this cell running. Stop it to shut down the server.")
print("=" * 70)

# Keep the process running
try:
    process.wait()
except KeyboardInterrupt:
    print("\n\nüõë Shutting down...")
    process.terminate()
    ngrok.disconnect(public_url.public_url)
    print("‚úÖ Server stopped")


# ============================================================================
# QUICK START SUMMARY
# ============================================================================
"""
‚úÖ CELL 1: Install packages - DONE
‚úÖ CELL 2: Create app file - Run this next
‚úÖ CELL 3: Configure ngrok token
‚úÖ CELL 4: Launch the app

After running all cells, you'll get a public URL to access your dashboard!
"""
